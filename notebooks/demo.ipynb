{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新闻视频处理演示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个笔记本中，我们将要针对一则来自江西湖口的最近一段时间本地新闻进行如下的处理：\n",
    "\n",
    "1. 按照 `cut-scene` 来切割出若干镜头\n",
    "\n",
    "1. 对上一步切割出来的镜头进行分类，识别出:\n",
    "\n",
    "  - 演播室（没有字幕）\n",
    "  - 演播室（带有字幕）\n",
    "  - 其它\n",
    "\n",
    "1. 识别和切割出完整的独立新闻视频\n",
    "\n",
    "1. 识别带有字幕的**非演播室**片段中的字幕文本\n",
    "\n",
    "1. 识别**演播室**片段中，播音员播报的语音内容，将它转为文本\n",
    "\n",
    "---\n",
    "\n",
    "> 之所以选择来自湖口的最近新闻录像，是因为：\n",
    ">\n",
    "> 在这个例子中，我们所使用到的若干神经网络模型，均只使用了少量湖口的新闻视频的人工标注样本进行训练，数据量较小，难以泛化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **❗ 重要：**\n",
    ">\n",
    "> 检查该 Notebook Web 界面的右上角，确定选择名为 `jxcn-newsvideo-poc` 的 `Kernel`\n",
    ">\n",
    "> 如果不是，可点击后在下拉列表中选中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入环境变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个笔记本的代码需要某些预定义的环境变量，所以 **必须** 加载。\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "\n",
    "%dotenv -o ../.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证 模型运行环境\n",
    "\n",
    "我们还需要：\n",
    "\n",
    "1. 验证 PyTorch 的情况：使用 CPU 还是 GPU\n",
    "\n",
    "1. 设置 matplotlib 中文字体：是否可以在绘图中显示中文\n",
    "\n",
    "   请根据实际情况设置有效的字体！\n",
    "\n",
    "1. 其他环境\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as mfm\n",
    "import torch\n",
    "\n",
    "from wasabi import Printer\n",
    "\n",
    "candidate_fonts = [\n",
    "    \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\",  # ubuntu\n",
    "    \"/usr/share/fonts/google-noto-cjk/NotoSansCJK-Regular.ttc\" # CentOS\n",
    "]\n",
    "\n",
    "msg = Printer()\n",
    "\n",
    "with msg.loading(\"CJK plotting test...\"):\n",
    "    mfm_font_path = ''\n",
    "    for font_path in candidate_fonts:\n",
    "        if os.path.exists(font_path):\n",
    "            mfm_font_path = font_path\n",
    "            break\n",
    "    if not mfm_font_path:\n",
    "        raise RuntimeError('没有找到列表中的 CJK 字体')\n",
    "    mfm_font_prop = mfm.FontProperties(fname=mfm_font_path)\n",
    "    plt.text(.5, .5, s='中文绘图显示测试', fontproperties=mfm_font_prop)\n",
    "    plt.pause(0.01)\n",
    "    plt.show()\n",
    "    plt.pause(0.01)\n",
    "\n",
    "with msg.loading(\"Torch device checking ...\"):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device\n",
    "    display(pd.DataFrame([{\n",
    "        'Pytorch Version': torch.__version__,\n",
    "        'Pytorch Device': device\n",
    "    }]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义全局函数\n",
    "\n",
    "定义几个用于 A/V 和图像处理的全局函数\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import base64\n",
    "import hashlib\n",
    "import json\n",
    "import urllib\n",
    "import subprocess\n",
    "from contextlib import contextmanager\n",
    "from copy import copy, deepcopy\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "from time import time, sleep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as mfm\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from matplotlib.patheffects import withStroke\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from wasabi import Printer\n",
    "\n",
    "from scenedetect.video_manager import VideoManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "from scenedetect.scene_manager import SceneManager\n",
    "from scenedetect.stats_manager import StatsManager\n",
    "\n",
    "from IPython.display import display, Image, Audio, Video, HTML\n",
    "\n",
    "\n",
    "msg = Printer()\n",
    "\n",
    "\n",
    "def run_ffmpeg(args=None, quiet=True, check=True, cmd=None):\n",
    "    if not cmd:\n",
    "        cmd = 'ffmpeg'\n",
    "    args = args or []\n",
    "    if quiet:\n",
    "        kv_args = dict(stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    else:\n",
    "        kv_args = dict()\n",
    "    subprocess.run([cmd, '-y'] + args, check=check, **kv_args)\n",
    "\n",
    "def trunc_av(video_file_path, start_seconds, end_seconds, output_path):\n",
    "    start_time = timedelta(seconds=start_seconds)\n",
    "    duration = timedelta(seconds=(end_seconds - start_seconds))\n",
    "    args = [\n",
    "        '-ss', str(start_time),\n",
    "        '-i', str(video_file_path),\n",
    "        '-t', str(duration),\n",
    "        '-c', 'copy',\n",
    "        # output:\n",
    "        str(output_path)\n",
    "    ]\n",
    "    run_ffmpeg(args=args, quiet=False)\n",
    "\n",
    "def trunc_a(video_file_path, start_seconds, end_seconds, output_path):\n",
    "    start_time = timedelta(seconds=start_seconds)\n",
    "    duration = timedelta(seconds=(end_seconds - start_seconds))\n",
    "    args = [\n",
    "        '-ss', str(start_time),\n",
    "        '-i', str(video_file_path),\n",
    "        '-t', str(duration),\n",
    "        '-vn', '-acodec', 'libopus', '-b:a', '16k',\n",
    "        # output:\n",
    "        str(output_path)\n",
    "    ]\n",
    "    run_ffmpeg(args=args)\n",
    "\n",
    "\n",
    "def add_inner_title(ax, title, loc, size=None, **kwargs):\n",
    "    prop = kwargs.get('prop', {})\n",
    "    prop['fontproperties'] = mfm_font_prop\n",
    "    if size is None:\n",
    "        prop['size'] = plt.rcParams['legend.fontsize']\n",
    "#     at = AnchoredText(title, loc=loc, pad=0., borderpad=0.5, frameon=False, **kwargs)\n",
    "    at = AnchoredText(title, loc=loc, prop=prop, pad=0., borderpad=0.5, frameon=False, **kwargs)\n",
    "    ax.add_artist(at)\n",
    "    at.txt._text.set_path_effects([withStroke(foreground=\"w\", linewidth=3)])\n",
    "    return at\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上传要处理的视频文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **十分重要 :**\n",
    ">\n",
    "> 请操作者自行上传湖口最近一段时间的新闻录像，并修改下面的变量为真实的文件路径\n",
    "\n",
    "上传一个湖口的新闻视频，并指定其路径。\n",
    "\n",
    "本例中，我们上传文件 *湖口* *2019年05月16日* 的新闻录像到:\n",
    "\n",
    "```sh\n",
    "../videos/湖口/20190516.mp4\n",
    "```\n",
    "\n",
    "> **提示：**\n",
    ">\n",
    "> 笔记本 Web 界面左侧的文件浏览器支持拖拽\n",
    "\n",
    "将文件路径赋值到全局变量 `VIDEO_DIR` 。后面的步骤需要使用该变量指定视频文件进行处理。\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "VIDEO_DIR = '../videos'  # 把视频上传到这个目录（相对于笔记本所在路径）\n",
    "\n",
    "# 定义要处理的视频文件路径之全局变量\n",
    "VIDEO_FILE = os.path.join(\n",
    "    VIDEO_DIR,\n",
    "    \"湖口/20190516.mp4\"  # 放到这个子目录！\n",
    ")\n",
    "\n",
    "print('要处理的视频：', os.path.abspath(VIDEO_FILE))  # 输出文件绝对路径\n",
    "assert os.path.exists(VIDEO_FILE)  # 断言：文件存在\n",
    "display(Video(VIDEO_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 场景边界预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一小节中，我们需要根据视觉表征进行场景边界预测\n",
    "\n",
    "在此之前，我们已经预先准备了一个使用根据视频内容表征进行视频场景边界检测的预训练模型。\n",
    "这个模型可以预测出视频场景边界。\n",
    "\n",
    "我们使用这个模型，对上传的新闻视频文件进行预测，得到它的场景边界，并显示缩略图\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.info('场景检测:')\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def managed_video_manager(*args, **kwargs):\n",
    "    video_manager = VideoManager(*args, **kwargs)\n",
    "    try:\n",
    "        yield video_manager\n",
    "    finally:\n",
    "        video_manager.release()\n",
    "\n",
    "def cut_scenes(video_file_path):\n",
    "    # cut-scenes 检测\n",
    "    stats_manager = StatsManager()\n",
    "    scene_manager = SceneManager(stats_manager)\n",
    "    scene_manager.add_detector(ContentDetector())\n",
    "    \n",
    "    with managed_video_manager([str(video_file_path)]) as video_manager:\n",
    "        # detect scenes\n",
    "        video_manager.start()\n",
    "        base_time_code = video_manager.get_base_timecode()\n",
    "        num_frames = scene_manager.detect_scenes(frame_source=video_manager)\n",
    "        if not num_frames > 0:\n",
    "            raise RuntimeError('No scenes detected')\n",
    "        scene_list = scene_manager.get_scene_list(base_time_code)\n",
    "    # Ok.\n",
    "    # 拼凑结果数据\n",
    "    scenes = []\n",
    "    for index, (start_time_code, end_time_code) in enumerate(scene_list):\n",
    "        scenes.append({\n",
    "            'index': index,\n",
    "            'start_time': start_time_code.get_seconds(),\n",
    "            'end_time': end_time_code.get_seconds(),\n",
    "            'start_frame': start_time_code.get_frames(),\n",
    "            'end_frame': end_time_code.get_frames(),\n",
    "        })\n",
    "    return scenes\n",
    "\n",
    "##\n",
    "\n",
    "scenes = cut_scenes(VIDEO_FILE)\n",
    "\n",
    "##########################################\n",
    "# 生成缩略图：\n",
    "\n",
    "from shutil import rmtree\n",
    "from datetime import timedelta\n",
    "from string import Template\n",
    "    \n",
    "def take_images_in_timespan(\n",
    "        video_file_path,\n",
    "        start_time,\n",
    "        end_time,\n",
    "        output_file_name_format='$index.jpg',\n",
    "        num=3\n",
    "):\n",
    "    output_tpl = Template(output_file_name_format)\n",
    "    time_points = []\n",
    "    img_file_list = []\n",
    "    if num == 1:\n",
    "        time_points.append(\n",
    "            start_time + (end_time - start_time) / 2\n",
    "        )\n",
    "    else:\n",
    "        span = (end_time - start_time) / (num - 1)\n",
    "        time_points.append(start_time)\n",
    "        for i in range(num - 2):\n",
    "            time_points.append(\n",
    "                start_time + span * (i + 1)\n",
    "            )\n",
    "        time_points.append(end_time)\n",
    "    #\n",
    "    for i, ss in enumerate(time_points):\n",
    "        ss = timedelta(seconds=ss)\n",
    "        output_path = output_tpl.substitute(index=i + 1)\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        run_ffmpeg([\n",
    "            '-ss', str(ss), '-i', video_file_path, '-vframes', '1', output_path\n",
    "        ])\n",
    "        img_file_list.append(output_path)\n",
    "    return img_file_list\n",
    "\n",
    "\n",
    "def take_scenes_images(video_file_path, scenes, output_dir):\n",
    "    # set output dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # take a frame as image\n",
    "    for i, scene in tqdm(enumerate(scenes), total=len(scenes)):\n",
    "        start_time = scene['start_time']\n",
    "        end_time = scene['end_time']\n",
    "        snapshots = take_images_in_timespan(\n",
    "            video_file_path, start_time, end_time,\n",
    "            os.path.join(output_dir, '{}_$index.jpg'.format(i + 1)),\n",
    "            1\n",
    "        )\n",
    "        scene['snapshots'] = snapshots\n",
    "\n",
    "\n",
    "dirname, basename = os.path.split(VIDEO_FILE)\n",
    "basename = os.path.splitext(basename)[0]\n",
    "images_dir = os.path.join(\n",
    "    '..',\n",
    "    'tmp',\n",
    "    'images',\n",
    "    os.path.relpath(dirname, VIDEO_DIR),\n",
    "    basename\n",
    ")\n",
    "rmtree(images_dir, ignore_errors=True)\n",
    "\n",
    "take_scenes_images(VIDEO_FILE, scenes, images_dir)\n",
    "\n",
    "###############################\n",
    "t0 = time()\n",
    "with msg.loading('Plotting ...'):\n",
    "    n_cols= 10\n",
    "    n_rows = int(np.ceil(len(scenes) / n_cols))\n",
    "    im_dsp_w = 80\n",
    "    im_dsp_h = 60\n",
    "\n",
    "    fig = plt.figure(figsize=(im_dsp_w, im_dsp_h))\n",
    "    grid = ImageGrid(\n",
    "        fig, len(scenes),  # similar to subplot(142)\n",
    "        nrows_ncols=(n_rows, n_cols),\n",
    "    )\n",
    "\n",
    "    text_prop={'fontproperties': mfm_font_prop}\n",
    "    for ax, scene in tqdm(zip(grid, scenes), total=len(scenes)):\n",
    "        inner_title = f\"场景[{scene['index']}]\\nfr: {timedelta(seconds=scene['start_time'])}\\nto: {timedelta(seconds=scene['end_time'])}\"\n",
    "        t = add_inner_title(ax, inner_title, loc=2)\n",
    "        t.patch.set_alpha(0.5)\n",
    "        im = mpimg.imread(scene['snapshots'][0])\n",
    "        ax.imshow(im, interpolation=\"nearest\")\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    plt.show()\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    msg.good(f'dur={timedelta(seconds=time()-t0)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 场景分类与组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经制作了一个小的，可进行对场景进行视频分类的预训练模型，它可以将场景分为：\n",
    "\n",
    "* `0`: 无字幕内容\n",
    "* `1`: 有字幕内容\n",
    "* `2`: 演播室\n",
    "\n",
    "之所以只进行如此简单的3个分类，是因为我们只需要在这个笔记本中进行几个单一功能的概念验证，选取少量数据进行标注和训练，可以节约时间。\n",
    "\n",
    "下面，我们进行分类，并根据分类结果重新组合场景。\n",
    "\n",
    "首先，进行上一步骤中划分的场景进行分类，并输出缩略图。\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.info('场景分类:')\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "IMAGE_CLASSES = {\n",
    "    0: '无字幕内容',\n",
    "    1: '有字幕内容',\n",
    "    2: '演播室',\n",
    "}\n",
    "\n",
    "\n",
    "INPUT_SIZE = 224\n",
    "\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(INPUT_SIZE),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(INPUT_SIZE),\n",
    "        transforms.CenterCrop(INPUT_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "def load_image_classify_model(path):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    if device.type == 'cpu':\n",
    "        model_ft = torch.load(path, map_location='cpu')\n",
    "        if isinstance(model_ft, nn.DataParallel):\n",
    "            model_ft = model_ft.module\n",
    "    else:\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_ft = torch.load(path)\n",
    "            model_ft = model_ft.to(device)\n",
    "    return model_ft\n",
    "\n",
    "\n",
    "\n",
    "def classify_scenes(scenes, model):\n",
    "    tsfrm = data_transforms['val']\n",
    "    samples_iter = (\n",
    "        tsfrm(transforms.ToPILImage()(skimage.io.imread(path)))\n",
    "        for path in (\n",
    "            scene['snapshots'][0]\n",
    "            for scene in scenes\n",
    "        )\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        batched_samples = torch.tensor([np.array(im) for im in samples_iter]).to(device)\n",
    "        predict_result = model(batched_samples)\n",
    "    \n",
    "    for scores, scene in zip(predict_result, scenes):\n",
    "        class_score = sorted(chain.from_iterable(zip(enumerate(scores))), key=lambda x: x[1], reverse=True)\n",
    "        top_class, top_score = class_score[0]\n",
    "        scene['class'] = top_class\n",
    "        scene['class_name'] = IMAGE_CLASSES[top_class]\n",
    "\n",
    "t0 = time()\n",
    "with msg.loading('classifying ...'):\n",
    "    image_classify_model_archive = '../models/squeezenet1_0-py36.pkl'\n",
    "    image_classify_model = load_image_classify_model(image_classify_model_archive)\n",
    "    classify_scenes(scenes, image_classify_model)\n",
    "    msg.good(f'dur={timedelta(seconds=time()-t0)}')\n",
    "\n",
    "######################################################\n",
    "t0 = time()\n",
    "with msg.loading('plotting ...'):\n",
    "    n_cols= 10\n",
    "    n_rows = int(np.ceil(len(scenes) / n_cols))\n",
    "    im_dsp_w = 80\n",
    "    im_dsp_h = 60\n",
    "\n",
    "    fig = plt.figure(figsize=(im_dsp_w, im_dsp_h))\n",
    "    grid = ImageGrid(\n",
    "        fig, len(scenes),  # similar to subplot(142)\n",
    "        nrows_ncols=(n_rows, n_cols),\n",
    "    )\n",
    "\n",
    "    for i, (ax, scene) in tqdm(enumerate(zip(grid, scenes)), total=len(scenes)):\n",
    "        inner_title = f\"{i+1}: {IMAGE_CLASSES[scene['class']]}\\nfr: {timedelta(seconds=scene['start_time'])}\\nto: {timedelta(seconds=scene['end_time'])}\"\n",
    "        t = add_inner_title(ax, inner_title, loc=2)\n",
    "        t.patch.set_alpha(0.5)\n",
    "        im = mpimg.imread(scene['snapshots'][0])\n",
    "        ax.imshow(im, interpolation=\"nearest\")\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    %time plt.show()\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    msg.good(f'dur={timedelta(seconds=time()-t0)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们按照视频场景分类信息重新组合视频片段。\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "msg.info('重组合视频片段:')\n",
    "\n",
    "\n",
    "# 合并类型相同的连续场景，去掉第一个演播室之前的\n",
    "\n",
    "merged_scenes = []\n",
    "\n",
    "for scene in scenes:\n",
    "    if not merged_scenes:\n",
    "        merged_scenes.append(deepcopy(scene))\n",
    "    else:\n",
    "        if merged_scenes[-1]['class'] == scene['class']:\n",
    "            merged_scenes[-1]['end_time'] = scene['end_time']\n",
    "        else:\n",
    "            merged_scenes.append(deepcopy(scene))\n",
    "\n",
    "while merged_scenes[0]['class'] != 2:\n",
    "    del merged_scenes[0]\n",
    "\n",
    "for i, scene in enumerate(merged_scenes):\n",
    "    scene['index'] = i\n",
    "\n",
    "###############################################\n",
    "##############################################################\n",
    "# 合并为故事片段\n",
    "\n",
    "stories = []\n",
    "\n",
    "for scene in merged_scenes:\n",
    "    scene_class = scene['class']\n",
    "    has_subtitle = scene_class == 1  # 是否有字幕\n",
    "    in_studio = scene_class == 2  #  是否在演播室\n",
    "    d = deepcopy(scene)\n",
    "    d['in_studio'] = in_studio\n",
    "    d['has_subtitle'] = has_subtitle\n",
    "\n",
    "    if not stories: # 第一个，总是添加\n",
    "        stories.append(d)\n",
    "\n",
    "    elif in_studio:  # 只要是演播室，就作为新的片段\n",
    "        stories.append(d)\n",
    "\n",
    "    elif has_subtitle:  #  有字幕，作为新的片段\n",
    "        stories.append(d)\n",
    "\n",
    "    else: # 这次没有字幕\n",
    "        if stories[-1]['in_studio']:  # 上一个是演播室，作为新的片段\n",
    "            stories.append(d)\n",
    "        else:  # 否则，上一个有字幕，合并片段\n",
    "            stories[-1]['end_time'] = scene['end_time']\n",
    "\n",
    "## 删除片尾：\n",
    "# del stories[-1]\n",
    "###########################################################################################################################\n",
    "# Segmenting AV clips\n",
    "dirname, basename = os.path.split(VIDEO_FILE)\n",
    "basename, extname = os.path.splitext(basename)\n",
    "clips_dir = os.path.join(\n",
    "    '..',\n",
    "    'tmp',\n",
    "    'clips',\n",
    "    os.path.relpath(dirname, VIDEO_DIR),\n",
    "    basename\n",
    ")\n",
    "rmtree(clips_dir, ignore_errors=True)\n",
    "\n",
    "for i, story in tqdm(enumerate(stories), total=len(stories)):\n",
    "    story['video_file'] = os.path.join(clips_dir, f'{i}{extname}')\n",
    "    os.makedirs(os.path.dirname(story['video_file']), exist_ok=True)\n",
    "\n",
    "    story['audio_file'] = os.path.join(clips_dir, f'{i}.opus')\n",
    "    os.makedirs(os.path.dirname(story['audio_file']), exist_ok=True)\n",
    "\n",
    "    trunc_av(VIDEO_FILE, story['start_time'], story['end_time'], story['video_file'])\n",
    "    trunc_a(VIDEO_FILE, story['start_time'], story['end_time'], story['audio_file'])\n",
    "\n",
    "###############\n",
    "_stories = [m for m in stories if m['has_subtitle']]\n",
    "\n",
    "t0 = time()\n",
    "with msg.loading('plotting ...'):\n",
    "    n_cols= 10\n",
    "    n_rows = int(np.ceil(len(_stories) / n_cols))\n",
    "    im_dsp_w = 80\n",
    "    im_dsp_h = 60\n",
    "\n",
    "    fig = plt.figure(figsize=(im_dsp_w, im_dsp_h))\n",
    "    grid = ImageGrid(\n",
    "        fig, len(scenes),  # similar to subplot(142)\n",
    "        nrows_ncols=(n_rows, n_cols),\n",
    "    )\n",
    "\n",
    "    for i, (ax, story) in tqdm(enumerate(zip(grid, _stories)), total=len(_stories)):\n",
    "        inner_title = f\"片段 {i+1}\\nfr: {timedelta(seconds=story['start_time'])}\\nto: {timedelta(seconds=story['end_time'])}\"\n",
    "        t = add_inner_title(ax, inner_title, loc=2)\n",
    "        t.patch.set_alpha(0.5)\n",
    "        im = mpimg.imread(story['snapshots'][0])\n",
    "        ax.imshow(im, interpolation=\"nearest\")\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    plt.show()\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    msg.good(f'dur={timedelta(seconds=time()-t0)}')\n",
    "    msg.good(f'共有： {len(_stories)}(条) 新闻片段')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了便于观察，我们输出这几个片段的视频：\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stories = [m for m in stories if m['has_subtitle']]\n",
    "# display\n",
    "for i, story in enumerate(_stories):\n",
    "    msg.info(\n",
    "        '片段 [{}]({} ~ {})'.format(\n",
    "            i+1,\n",
    "            timedelta(seconds=story['start_time']),\n",
    "            timedelta(seconds=story['end_time']))\n",
    "    )\n",
    "    display(Video(story['video_file']))\n",
    "    print('-' * 100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语音识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 *场景分类与组合* 一节中，被预测到是演播室的片段，将新闻主播的语音识别为文本\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.info('语音识别:')\n",
    "\n",
    "\n",
    "import weblfasr_python3_demo as lfasr\n",
    "\n",
    "\n",
    "_stories = [m for m in stories if m['in_studio']]\n",
    "\n",
    "for story in tqdm(_stories):\n",
    "    asr_req = lfasr.RequestApi(os.getenv(\"XFYUN_APP_ID\"), os.getenv(\"XFYUN_SECRET_KEY\"), story['audio_file'])\n",
    "    res = asr_req.all_api_request()\n",
    "    asr_res_data = json.loads(res['data'])\n",
    "    story['asr-result'] = asr_res_data\n",
    "\n",
    "msg.good('语音识别完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "识别完成后，我们输出视频片段与识别结果，进行评估：\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for story in _stories:\n",
    "    display(Video(story['video_file']))\n",
    "    text = ' '.join(m['onebest'] for m in story['asr-result'])\n",
    "    print('语音内容：')\n",
    "    print(text)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字幕识别（新闻标题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 *场景分类* 一节中，被预测到具有字幕的内容，将视频中的字幕识别为文本\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.info('字幕识别:')\n",
    "\n",
    "\n",
    "SUBTITLE_H_PENCENTS = (0.8, 0.98)\n",
    "\n",
    "def crop_for_subtitle(arr):\n",
    "    h, _, _ = arr.shape\n",
    "    h_fr, h_to = np.ceil(np.repeat(h, 2) * SUBTITLE_H_PENCENTS).astype(int)\n",
    "    return arr[h_fr:h_to, :, :]\n",
    "\n",
    "\n",
    "def im_to_b64(arr, ext_name='.jpeg'):\n",
    "    from tempfile import mkstemp\n",
    "    from base64 import b64encode\n",
    "    \n",
    "    _, path = mkstemp(suffix=ext_name)\n",
    "    try:\n",
    "        plt.imsave(path, arr)\n",
    "        with open(path, 'rb') as fp:\n",
    "            data = fp.read()\n",
    "    finally:\n",
    "        os.remove(path)\n",
    "    return b64encode(data).decode()\n",
    "\n",
    "def ocr_subtitle(im_path):\n",
    "    url = os.getenv('OCR_API_URL')\n",
    "    pid = os.getenv('OCR_API_PID')\n",
    "    key = os.getenv('OCR_API_KEY')\n",
    "    service = os.getenv('OCR_API_SERVICE')\n",
    "    salt = os.getenv('OCR_API_SALT')\n",
    "\n",
    "    def _md5(s):\n",
    "        m = hashlib.md5()\n",
    "        m.update(s.encode(\"utf8\"))\n",
    "        return m.hexdigest()\n",
    "    \n",
    "    arr = plt.imread(im_path)\n",
    "    arr = crop_for_subtitle(arr)\n",
    "\n",
    "    image_base64_str = im_to_b64(arr)\n",
    "    image_short = image_base64_str[0:1024]\n",
    "    sign = _md5(pid + service + salt + image_short + key)\n",
    "\n",
    "    data = {\n",
    "        'pid': pid,\n",
    "        'service': service,\n",
    "        'sign': sign,\n",
    "        'salt': salt,\n",
    "        'image': image_base64_str,\n",
    "    }\n",
    "    headers = {\n",
    "        'accept': \"application/json\"\n",
    "    }\n",
    "    res = requests.request(\"POST\", url, data=data, headers=headers)\n",
    "    res = res.json()\n",
    "\n",
    "    if(res[\"success\"]) == 1:\n",
    "        result = res['result']\n",
    "        return result\n",
    "    else:\n",
    "        raise RuntimeError(\"OCR ERROR\")\n",
    "\n",
    "        \n",
    "_stories = [m for m in stories if m['has_subtitle']]\n",
    "        \n",
    "for story in tqdm(_stories):\n",
    "    im_path = story['snapshots'][0]\n",
    "    story['ocr_result'] = ocr_subtitle(im_path)\n",
    "\n",
    "msg.good('字幕识别完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "识别完毕之后，我们输出结果文本与视频、图像，进行观察比对：\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, story in enumerate(_stories):\n",
    "    display(Video(story['video_file']))\n",
    "\n",
    "    print(f\"片段 {i+1} ({timedelta(seconds=story['start_time'])} ～ {timedelta(seconds=story['end_time'])})\")\n",
    "    print('字幕：')\n",
    "    ocr_result = story['ocr_result']\n",
    "    print(' | '.join(m['content'].strip() for m in ocr_result))\n",
    "    \n",
    "    im_path = story['snapshots'][0]\n",
    "    fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "    im = plt.imread(im_path)\n",
    "    h, _, _ = im.shape\n",
    "    ax.imshow(im)\n",
    "    for m in ocr_result:\n",
    "        text_rect = [list(map(int, s.split(','))) for s in m['frame']]\n",
    "        left, bottom = text_rect[0][0], text_rect[3][1]\n",
    "        width = text_rect[1][0] - text_rect[0][0]\n",
    "        height = text_rect[3][1] - text_rect[0][1]\n",
    "        bottom += h * SUBTITLE_H_PENCENTS[0] - height\n",
    "        rect = mpatches.Rectangle((left, bottom),width,height,linewidth=2.5,edgecolor='yellow',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "    plt.pause(1)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最终结果呈现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终，我们得到了新闻现场视屏的片段及其标题，演播室片段及其语音内容，下面把它们列表呈现出来。\n",
    "\n",
    "**执行代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for story in stories:\n",
    "    if story['in_studio']:\n",
    "        display(HTML('<br/><hr/>'))\n",
    "        msg.info('演播室', f\"{timedelta(seconds=story['start_time'])} ～ {timedelta(seconds=story['end_time'])}\")\n",
    "        display(Video(story['video_file']))\n",
    "        display(HTML('<strong><mark>语音：</mark></strong>'))\n",
    "        text = ' '.join(m['onebest'] for m in story['asr-result'])\n",
    "        display(HTML(f'<mark>{text}</mark>'))\n",
    "    elif story['has_subtitle']:\n",
    "        display(HTML('<hr/>'))\n",
    "        msg.info('新闻内容', f\"{timedelta(seconds=story['start_time'])} ～ {timedelta(seconds=story['end_time'])}\")\n",
    "        display(HTML('<strong><mark>标题：</mark></strong>'))\n",
    "        text = max(\n",
    "            (m['content'].strip() for m in story['ocr_result']),\n",
    "            key=lambda s: len(s)\n",
    "        )\n",
    "        display(HTML(f'<mark>{text}</mark>'))\n",
    "        display(Video(story['video_file']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jxcn-newsvideo-poc]",
   "language": "python",
   "name": "conda-env-jxcn-newsvideo-poc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
